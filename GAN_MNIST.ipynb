{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "\n",
    "# 생성자(Generator) 클래스 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # 하나의 블록(block) 정의\n",
    "        def block(input_dim, output_dim, normalize=True):\n",
    "            layers = [nn.Linear(input_dim, output_dim)]\n",
    "            if normalize:\n",
    "                # 배치 정규화(batch normalization) 수행(차원 동일)\n",
    "                layers.append(nn.BatchNorm1d(output_dim, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        # 생성자 모델은 연속적인 여러 개의 블록을 가짐\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, 1 * 28 * 28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판별자(Discriminator) 클래스 정의\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1 * 28 * 28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # 이미지에 대한 판별 결과를 반환\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(img.size(0), -1)\n",
    "        output = self.model(flattened)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 불러오기\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./dataset', train=True, download=True, transform=transforms_train)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 샘플링\n",
    "# 학습을 위해 생성자와 판별자모델 초기화\n",
    "# 적절한 하이퍼 파라미터 설정\n",
    "\n",
    "# initialize Generator & Discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.cuda()\n",
    "discriminator.cuda()\n",
    "\n",
    "# loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "adversarial_loss.cuda()\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.0002\n",
    "\n",
    "# Optimizer for G & D\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [D loss: 0.394058] [G loss: 1.158801] [Elapsed time: 10.84s]\n",
      "[Epoch 1/200] [D loss: 0.434890] [G loss: 0.868250] [Elapsed time: 19.95s]\n",
      "[Epoch 2/200] [D loss: 0.335985] [G loss: 1.216575] [Elapsed time: 30.39s]\n",
      "[Epoch 3/200] [D loss: 0.425727] [G loss: 0.783289] [Elapsed time: 41.42s]\n",
      "[Epoch 4/200] [D loss: 0.341921] [G loss: 2.612060] [Elapsed time: 50.35s]\n",
      "[Epoch 5/200] [D loss: 0.295815] [G loss: 1.405692] [Elapsed time: 59.52s]\n",
      "[Epoch 6/200] [D loss: 0.310228] [G loss: 1.158691] [Elapsed time: 67.33s]\n",
      "[Epoch 7/200] [D loss: 0.209788] [G loss: 1.993060] [Elapsed time: 75.26s]\n",
      "[Epoch 8/200] [D loss: 0.139987] [G loss: 2.279531] [Elapsed time: 84.42s]\n",
      "[Epoch 9/200] [D loss: 0.226051] [G loss: 1.272905] [Elapsed time: 94.87s]\n",
      "[Epoch 10/200] [D loss: 0.297232] [G loss: 0.997724] [Elapsed time: 102.96s]\n",
      "[Epoch 11/200] [D loss: 0.269333] [G loss: 1.308645] [Elapsed time: 110.74s]\n",
      "[Epoch 12/200] [D loss: 0.368887] [G loss: 0.824788] [Elapsed time: 118.88s]\n",
      "[Epoch 13/200] [D loss: 0.287732] [G loss: 2.925611] [Elapsed time: 126.69s]\n",
      "[Epoch 14/200] [D loss: 0.160027] [G loss: 2.606640] [Elapsed time: 134.72s]\n",
      "[Epoch 15/200] [D loss: 0.247981] [G loss: 2.651962] [Elapsed time: 142.53s]\n",
      "[Epoch 16/200] [D loss: 0.248327] [G loss: 1.641176] [Elapsed time: 150.53s]\n",
      "[Epoch 17/200] [D loss: 0.096588] [G loss: 2.898682] [Elapsed time: 158.88s]\n",
      "[Epoch 18/200] [D loss: 0.203026] [G loss: 1.840033] [Elapsed time: 168.15s]\n",
      "[Epoch 19/200] [D loss: 0.166289] [G loss: 2.453491] [Elapsed time: 177.58s]\n",
      "[Epoch 20/200] [D loss: 0.176604] [G loss: 1.681995] [Elapsed time: 186.84s]\n",
      "[Epoch 21/200] [D loss: 0.168330] [G loss: 5.499161] [Elapsed time: 194.84s]\n",
      "[Epoch 22/200] [D loss: 0.225921] [G loss: 3.879585] [Elapsed time: 202.87s]\n",
      "[Epoch 23/200] [D loss: 0.255763] [G loss: 5.512604] [Elapsed time: 210.66s]\n",
      "[Epoch 24/200] [D loss: 0.173915] [G loss: 1.766010] [Elapsed time: 218.66s]\n",
      "[Epoch 25/200] [D loss: 0.379189] [G loss: 6.454123] [Elapsed time: 226.74s]\n",
      "[Epoch 26/200] [D loss: 0.140911] [G loss: 2.354232] [Elapsed time: 234.72s]\n",
      "[Epoch 27/200] [D loss: 0.170817] [G loss: 2.004575] [Elapsed time: 242.52s]\n",
      "[Epoch 28/200] [D loss: 0.233950] [G loss: 3.816500] [Elapsed time: 250.48s]\n",
      "[Epoch 29/200] [D loss: 0.201065] [G loss: 3.359878] [Elapsed time: 258.32s]\n",
      "[Epoch 30/200] [D loss: 0.040750] [G loss: 4.210599] [Elapsed time: 268.15s]\n",
      "[Epoch 31/200] [D loss: 0.370580] [G loss: 0.905781] [Elapsed time: 278.47s]\n",
      "[Epoch 32/200] [D loss: 0.154735] [G loss: 2.043491] [Elapsed time: 286.28s]\n",
      "[Epoch 33/200] [D loss: 0.133770] [G loss: 2.761242] [Elapsed time: 294.37s]\n",
      "[Epoch 34/200] [D loss: 0.104161] [G loss: 2.646144] [Elapsed time: 302.23s]\n",
      "[Epoch 35/200] [D loss: 0.123965] [G loss: 3.194057] [Elapsed time: 310.27s]\n",
      "[Epoch 36/200] [D loss: 0.844338] [G loss: 8.397648] [Elapsed time: 318.09s]\n",
      "[Epoch 37/200] [D loss: 0.134821] [G loss: 4.745570] [Elapsed time: 326.42s]\n",
      "[Epoch 38/200] [D loss: 0.229082] [G loss: 3.567418] [Elapsed time: 334.45s]\n",
      "[Epoch 39/200] [D loss: 0.272579] [G loss: 1.115815] [Elapsed time: 342.36s]\n",
      "[Epoch 40/200] [D loss: 0.125465] [G loss: 2.996978] [Elapsed time: 350.08s]\n",
      "[Epoch 41/200] [D loss: 0.130191] [G loss: 3.020694] [Elapsed time: 358.08s]\n",
      "[Epoch 42/200] [D loss: 0.042756] [G loss: 5.582417] [Elapsed time: 365.91s]\n",
      "[Epoch 43/200] [D loss: 0.144443] [G loss: 2.308889] [Elapsed time: 373.81s]\n",
      "[Epoch 44/200] [D loss: 0.253590] [G loss: 1.334823] [Elapsed time: 381.59s]\n",
      "[Epoch 45/200] [D loss: 0.144192] [G loss: 3.507728] [Elapsed time: 389.41s]\n",
      "[Epoch 46/200] [D loss: 0.095018] [G loss: 3.223166] [Elapsed time: 397.23s]\n",
      "[Epoch 47/200] [D loss: 0.075428] [G loss: 2.389400] [Elapsed time: 405.04s]\n",
      "[Epoch 48/200] [D loss: 0.530361] [G loss: 2.508949] [Elapsed time: 413.03s]\n",
      "[Epoch 49/200] [D loss: 0.172309] [G loss: 1.587754] [Elapsed time: 420.75s]\n",
      "[Epoch 50/200] [D loss: 0.184898] [G loss: 2.345975] [Elapsed time: 428.89s]\n",
      "[Epoch 51/200] [D loss: 0.533546] [G loss: 7.936873] [Elapsed time: 437.07s]\n",
      "[Epoch 52/200] [D loss: 0.171642] [G loss: 2.373929] [Elapsed time: 445.07s]\n",
      "[Epoch 53/200] [D loss: 0.148683] [G loss: 2.992785] [Elapsed time: 452.87s]\n",
      "[Epoch 54/200] [D loss: 0.076605] [G loss: 3.764987] [Elapsed time: 460.82s]\n",
      "[Epoch 55/200] [D loss: 0.170568] [G loss: 1.939074] [Elapsed time: 468.63s]\n",
      "[Epoch 56/200] [D loss: 0.135568] [G loss: 3.067703] [Elapsed time: 476.58s]\n",
      "[Epoch 57/200] [D loss: 0.124782] [G loss: 2.130838] [Elapsed time: 484.28s]\n",
      "[Epoch 58/200] [D loss: 0.182958] [G loss: 3.261284] [Elapsed time: 495.07s]\n",
      "[Epoch 59/200] [D loss: 0.099215] [G loss: 2.623131] [Elapsed time: 507.03s]\n",
      "[Epoch 60/200] [D loss: 0.172675] [G loss: 1.805485] [Elapsed time: 516.14s]\n",
      "[Epoch 61/200] [D loss: 0.096947] [G loss: 3.396308] [Elapsed time: 526.08s]\n",
      "[Epoch 62/200] [D loss: 0.186308] [G loss: 1.961277] [Elapsed time: 536.67s]\n",
      "[Epoch 63/200] [D loss: 0.129369] [G loss: 2.514124] [Elapsed time: 545.09s]\n",
      "[Epoch 64/200] [D loss: 0.430604] [G loss: 1.699514] [Elapsed time: 553.54s]\n",
      "[Epoch 65/200] [D loss: 0.180775] [G loss: 2.700067] [Elapsed time: 562.46s]\n",
      "[Epoch 66/200] [D loss: 0.066561] [G loss: 3.794848] [Elapsed time: 572.15s]\n",
      "[Epoch 67/200] [D loss: 0.197787] [G loss: 2.916276] [Elapsed time: 582.65s]\n",
      "[Epoch 68/200] [D loss: 0.150514] [G loss: 2.068630] [Elapsed time: 592.73s]\n",
      "[Epoch 69/200] [D loss: 0.107090] [G loss: 2.842458] [Elapsed time: 601.72s]\n",
      "[Epoch 70/200] [D loss: 0.167492] [G loss: 3.373213] [Elapsed time: 609.98s]\n",
      "[Epoch 71/200] [D loss: 0.128678] [G loss: 2.633608] [Elapsed time: 617.94s]\n",
      "[Epoch 72/200] [D loss: 0.094928] [G loss: 4.074414] [Elapsed time: 625.72s]\n",
      "[Epoch 73/200] [D loss: 0.148881] [G loss: 6.066941] [Elapsed time: 633.67s]\n",
      "[Epoch 74/200] [D loss: 0.294416] [G loss: 1.406832] [Elapsed time: 641.46s]\n",
      "[Epoch 75/200] [D loss: 0.143282] [G loss: 2.455317] [Elapsed time: 649.42s]\n",
      "[Epoch 76/200] [D loss: 0.146454] [G loss: 2.084156] [Elapsed time: 657.41s]\n",
      "[Epoch 77/200] [D loss: 0.098466] [G loss: 3.302461] [Elapsed time: 665.57s]\n",
      "[Epoch 78/200] [D loss: 0.130355] [G loss: 2.825548] [Elapsed time: 673.88s]\n",
      "[Epoch 79/200] [D loss: 0.191762] [G loss: 2.521685] [Elapsed time: 682.31s]\n",
      "[Epoch 80/200] [D loss: 0.174621] [G loss: 2.167344] [Elapsed time: 690.02s]\n",
      "[Epoch 81/200] [D loss: 0.259261] [G loss: 4.359153] [Elapsed time: 698.13s]\n",
      "[Epoch 82/200] [D loss: 0.129044] [G loss: 2.183195] [Elapsed time: 705.84s]\n",
      "[Epoch 83/200] [D loss: 0.099481] [G loss: 3.422094] [Elapsed time: 713.80s]\n",
      "[Epoch 84/200] [D loss: 0.159713] [G loss: 4.039025] [Elapsed time: 721.50s]\n",
      "[Epoch 85/200] [D loss: 0.140845] [G loss: 4.172763] [Elapsed time: 729.61s]\n",
      "[Epoch 86/200] [D loss: 0.119032] [G loss: 3.278238] [Elapsed time: 737.30s]\n",
      "[Epoch 87/200] [D loss: 0.113649] [G loss: 2.561975] [Elapsed time: 746.80s]\n",
      "[Epoch 88/200] [D loss: 0.119491] [G loss: 2.766424] [Elapsed time: 755.36s]\n",
      "[Epoch 89/200] [D loss: 0.125089] [G loss: 3.533601] [Elapsed time: 764.92s]\n",
      "[Epoch 90/200] [D loss: 0.145143] [G loss: 5.580847] [Elapsed time: 772.75s]\n",
      "[Epoch 91/200] [D loss: 0.156864] [G loss: 3.233799] [Elapsed time: 780.67s]\n",
      "[Epoch 92/200] [D loss: 0.160329] [G loss: 2.574443] [Elapsed time: 788.44s]\n",
      "[Epoch 93/200] [D loss: 0.157927] [G loss: 2.347435] [Elapsed time: 800.55s]\n",
      "[Epoch 94/200] [D loss: 0.294586] [G loss: 1.484736] [Elapsed time: 812.59s]\n",
      "[Epoch 95/200] [D loss: 0.155740] [G loss: 2.796238] [Elapsed time: 822.91s]\n",
      "[Epoch 96/200] [D loss: 0.107042] [G loss: 2.898499] [Elapsed time: 831.06s]\n",
      "[Epoch 97/200] [D loss: 0.081627] [G loss: 2.426377] [Elapsed time: 840.82s]\n",
      "[Epoch 98/200] [D loss: 0.189267] [G loss: 1.987187] [Elapsed time: 852.51s]\n",
      "[Epoch 99/200] [D loss: 0.149412] [G loss: 2.407397] [Elapsed time: 862.58s]\n",
      "[Epoch 100/200] [D loss: 0.106406] [G loss: 2.515731] [Elapsed time: 872.81s]\n",
      "[Epoch 101/200] [D loss: 0.119378] [G loss: 2.637374] [Elapsed time: 881.81s]\n",
      "[Epoch 102/200] [D loss: 0.141275] [G loss: 2.844393] [Elapsed time: 892.18s]\n",
      "[Epoch 103/200] [D loss: 0.169038] [G loss: 2.531818] [Elapsed time: 900.77s]\n",
      "[Epoch 104/200] [D loss: 0.234913] [G loss: 2.378249] [Elapsed time: 909.30s]\n",
      "[Epoch 105/200] [D loss: 0.217853] [G loss: 3.473313] [Elapsed time: 917.36s]\n",
      "[Epoch 106/200] [D loss: 0.179437] [G loss: 1.556614] [Elapsed time: 925.50s]\n",
      "[Epoch 107/200] [D loss: 0.142430] [G loss: 2.600613] [Elapsed time: 935.44s]\n",
      "[Epoch 108/200] [D loss: 0.335150] [G loss: 1.634340] [Elapsed time: 944.66s]\n",
      "[Epoch 109/200] [D loss: 0.095940] [G loss: 2.861313] [Elapsed time: 953.81s]\n",
      "[Epoch 110/200] [D loss: 0.146123] [G loss: 2.632671] [Elapsed time: 962.91s]\n",
      "[Epoch 111/200] [D loss: 0.161362] [G loss: 2.926296] [Elapsed time: 971.69s]\n",
      "[Epoch 112/200] [D loss: 0.246885] [G loss: 2.637814] [Elapsed time: 979.56s]\n",
      "[Epoch 113/200] [D loss: 0.109930] [G loss: 3.064255] [Elapsed time: 988.16s]\n",
      "[Epoch 114/200] [D loss: 0.229423] [G loss: 1.994419] [Elapsed time: 997.22s]\n",
      "[Epoch 115/200] [D loss: 0.063449] [G loss: 3.492949] [Elapsed time: 1006.65s]\n",
      "[Epoch 116/200] [D loss: 0.110939] [G loss: 2.195751] [Elapsed time: 1016.56s]\n",
      "[Epoch 117/200] [D loss: 0.124527] [G loss: 2.870887] [Elapsed time: 1026.34s]\n",
      "[Epoch 118/200] [D loss: 0.170220] [G loss: 2.152559] [Elapsed time: 1034.66s]\n",
      "[Epoch 119/200] [D loss: 0.113247] [G loss: 2.873326] [Elapsed time: 1043.01s]\n",
      "[Epoch 120/200] [D loss: 0.192788] [G loss: 1.726638] [Elapsed time: 1051.37s]\n",
      "[Epoch 121/200] [D loss: 0.212879] [G loss: 3.886400] [Elapsed time: 1061.05s]\n",
      "[Epoch 122/200] [D loss: 0.233965] [G loss: 3.602522] [Elapsed time: 1070.56s]\n",
      "[Epoch 123/200] [D loss: 0.139930] [G loss: 3.344034] [Elapsed time: 1080.27s]\n",
      "[Epoch 124/200] [D loss: 0.319670] [G loss: 7.565614] [Elapsed time: 1088.62s]\n",
      "[Epoch 125/200] [D loss: 0.121862] [G loss: 3.894736] [Elapsed time: 1097.38s]\n",
      "[Epoch 126/200] [D loss: 0.086923] [G loss: 4.045836] [Elapsed time: 1106.39s]\n",
      "[Epoch 127/200] [D loss: 0.085608] [G loss: 3.061403] [Elapsed time: 1115.47s]\n",
      "[Epoch 128/200] [D loss: 0.120333] [G loss: 3.412513] [Elapsed time: 1125.35s]\n",
      "[Epoch 129/200] [D loss: 0.129510] [G loss: 2.146286] [Elapsed time: 1135.09s]\n",
      "[Epoch 130/200] [D loss: 0.161112] [G loss: 3.610812] [Elapsed time: 1143.69s]\n",
      "[Epoch 131/200] [D loss: 0.126889] [G loss: 2.349948] [Elapsed time: 1152.94s]\n",
      "[Epoch 132/200] [D loss: 0.185094] [G loss: 2.683825] [Elapsed time: 1163.52s]\n",
      "[Epoch 133/200] [D loss: 0.182281] [G loss: 4.531180] [Elapsed time: 1175.83s]\n",
      "[Epoch 134/200] [D loss: 0.136709] [G loss: 3.952583] [Elapsed time: 1185.53s]\n",
      "[Epoch 135/200] [D loss: 0.106903] [G loss: 3.427796] [Elapsed time: 1194.87s]\n",
      "[Epoch 136/200] [D loss: 0.190238] [G loss: 4.887016] [Elapsed time: 1204.74s]\n",
      "[Epoch 137/200] [D loss: 0.108227] [G loss: 2.694944] [Elapsed time: 1214.61s]\n",
      "[Epoch 138/200] [D loss: 0.143017] [G loss: 2.645675] [Elapsed time: 1223.58s]\n",
      "[Epoch 139/200] [D loss: 0.079739] [G loss: 2.960074] [Elapsed time: 1233.49s]\n",
      "[Epoch 140/200] [D loss: 0.684901] [G loss: 9.599441] [Elapsed time: 1243.28s]\n",
      "[Epoch 141/200] [D loss: 0.175446] [G loss: 2.812912] [Elapsed time: 1252.22s]\n",
      "[Epoch 142/200] [D loss: 0.124919] [G loss: 2.770519] [Elapsed time: 1261.28s]\n",
      "[Epoch 143/200] [D loss: 0.205501] [G loss: 5.296514] [Elapsed time: 1270.58s]\n",
      "[Epoch 144/200] [D loss: 0.183509] [G loss: 3.141148] [Elapsed time: 1278.63s]\n",
      "[Epoch 145/200] [D loss: 0.138477] [G loss: 2.848655] [Elapsed time: 1287.29s]\n",
      "[Epoch 146/200] [D loss: 0.128205] [G loss: 2.433033] [Elapsed time: 1295.78s]\n",
      "[Epoch 147/200] [D loss: 0.117004] [G loss: 2.812915] [Elapsed time: 1305.32s]\n",
      "[Epoch 148/200] [D loss: 0.085661] [G loss: 4.054957] [Elapsed time: 1315.35s]\n",
      "[Epoch 149/200] [D loss: 0.185086] [G loss: 3.114304] [Elapsed time: 1325.93s]\n",
      "[Epoch 150/200] [D loss: 0.255144] [G loss: 2.956664] [Elapsed time: 1335.19s]\n",
      "[Epoch 151/200] [D loss: 0.154332] [G loss: 2.427741] [Elapsed time: 1344.65s]\n",
      "[Epoch 152/200] [D loss: 0.215194] [G loss: 1.966575] [Elapsed time: 1355.34s]\n",
      "[Epoch 153/200] [D loss: 0.180001] [G loss: 5.542187] [Elapsed time: 1364.50s]\n",
      "[Epoch 154/200] [D loss: 0.102366] [G loss: 4.460152] [Elapsed time: 1376.90s]\n",
      "[Epoch 155/200] [D loss: 0.189428] [G loss: 3.146321] [Elapsed time: 1386.74s]\n",
      "[Epoch 156/200] [D loss: 0.074307] [G loss: 4.064641] [Elapsed time: 1396.16s]\n",
      "[Epoch 157/200] [D loss: 0.090741] [G loss: 2.745494] [Elapsed time: 1407.31s]\n",
      "[Epoch 158/200] [D loss: 0.109808] [G loss: 2.534019] [Elapsed time: 1418.22s]\n",
      "[Epoch 159/200] [D loss: 0.181581] [G loss: 6.046781] [Elapsed time: 1427.54s]\n",
      "[Epoch 160/200] [D loss: 0.069611] [G loss: 3.361678] [Elapsed time: 1435.85s]\n",
      "[Epoch 161/200] [D loss: 0.269990] [G loss: 2.136781] [Elapsed time: 1447.89s]\n",
      "[Epoch 162/200] [D loss: 0.165373] [G loss: 2.915436] [Elapsed time: 1459.08s]\n",
      "[Epoch 163/200] [D loss: 0.220909] [G loss: 4.702756] [Elapsed time: 1471.27s]\n",
      "[Epoch 164/200] [D loss: 0.291865] [G loss: 7.800239] [Elapsed time: 1483.62s]\n",
      "[Epoch 165/200] [D loss: 0.107530] [G loss: 3.253573] [Elapsed time: 1494.06s]\n",
      "[Epoch 166/200] [D loss: 0.087506] [G loss: 4.656793] [Elapsed time: 1503.09s]\n",
      "[Epoch 167/200] [D loss: 0.201441] [G loss: 2.687454] [Elapsed time: 1511.37s]\n",
      "[Epoch 168/200] [D loss: 0.077127] [G loss: 5.581450] [Elapsed time: 1519.19s]\n",
      "[Epoch 169/200] [D loss: 0.206124] [G loss: 1.905743] [Elapsed time: 1527.43s]\n",
      "[Epoch 170/200] [D loss: 0.181827] [G loss: 2.083383] [Elapsed time: 1535.51s]\n",
      "[Epoch 171/200] [D loss: 0.335798] [G loss: 5.122849] [Elapsed time: 1543.53s]\n",
      "[Epoch 172/200] [D loss: 0.146413] [G loss: 2.185039] [Elapsed time: 1551.34s]\n",
      "[Epoch 173/200] [D loss: 0.162584] [G loss: 2.593087] [Elapsed time: 1559.60s]\n",
      "[Epoch 174/200] [D loss: 0.173993] [G loss: 3.919391] [Elapsed time: 1569.20s]\n",
      "[Epoch 175/200] [D loss: 0.167877] [G loss: 2.935825] [Elapsed time: 1578.27s]\n",
      "[Epoch 176/200] [D loss: 0.151776] [G loss: 2.381711] [Elapsed time: 1588.00s]\n",
      "[Epoch 177/200] [D loss: 0.139705] [G loss: 4.213334] [Elapsed time: 1598.17s]\n",
      "[Epoch 178/200] [D loss: 0.141516] [G loss: 2.595837] [Elapsed time: 1607.64s]\n",
      "[Epoch 179/200] [D loss: 0.136606] [G loss: 2.486140] [Elapsed time: 1616.74s]\n",
      "[Epoch 180/200] [D loss: 0.055648] [G loss: 3.775461] [Elapsed time: 1625.62s]\n",
      "[Epoch 181/200] [D loss: 0.180551] [G loss: 5.494108] [Elapsed time: 1634.16s]\n",
      "[Epoch 182/200] [D loss: 0.184890] [G loss: 3.907048] [Elapsed time: 1643.94s]\n",
      "[Epoch 183/200] [D loss: 0.057543] [G loss: 4.155859] [Elapsed time: 1653.85s]\n",
      "[Epoch 184/200] [D loss: 0.067564] [G loss: 3.663376] [Elapsed time: 1662.15s]\n",
      "[Epoch 185/200] [D loss: 0.193559] [G loss: 5.105789] [Elapsed time: 1670.44s]\n",
      "[Epoch 186/200] [D loss: 0.076449] [G loss: 3.123552] [Elapsed time: 1678.36s]\n",
      "[Epoch 187/200] [D loss: 0.066572] [G loss: 3.621252] [Elapsed time: 1688.36s]\n",
      "[Epoch 188/200] [D loss: 0.083815] [G loss: 3.567684] [Elapsed time: 1699.81s]\n",
      "[Epoch 189/200] [D loss: 0.062903] [G loss: 4.523151] [Elapsed time: 1709.08s]\n",
      "[Epoch 190/200] [D loss: 0.132497] [G loss: 3.358872] [Elapsed time: 1718.69s]\n",
      "[Epoch 191/200] [D loss: 0.149046] [G loss: 5.032390] [Elapsed time: 1727.48s]\n",
      "[Epoch 192/200] [D loss: 0.134902] [G loss: 3.149814] [Elapsed time: 1736.93s]\n",
      "[Epoch 193/200] [D loss: 0.157176] [G loss: 2.804361] [Elapsed time: 1745.47s]\n",
      "[Epoch 194/200] [D loss: 0.166255] [G loss: 7.010961] [Elapsed time: 1754.31s]\n",
      "[Epoch 195/200] [D loss: 0.306983] [G loss: 0.940196] [Elapsed time: 1762.76s]\n",
      "[Epoch 196/200] [D loss: 0.133424] [G loss: 4.218963] [Elapsed time: 1772.81s]\n",
      "[Epoch 197/200] [D loss: 0.164475] [G loss: 5.381824] [Elapsed time: 1784.55s]\n",
      "[Epoch 198/200] [D loss: 0.140139] [G loss: 2.550573] [Elapsed time: 1796.94s]\n",
      "[Epoch 199/200] [D loss: 0.136105] [G loss: 3.707605] [Elapsed time: 1808.23s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 200 # 학습의 횟수(epoch) 설정\n",
    "sample_interval = 2000 # 몇 번의 배치(batch)마다 결과를 출력할 것인지 설정\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # 진짜(real) 이미지와 가짜(fake) 이미지에 대한 정답 레이블 생성\n",
    "        real = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(1.0) # 진짜(real): 1\n",
    "        fake = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(0.0) # 가짜(fake): 0\n",
    "\n",
    "        real_imgs = imgs.cuda()\n",
    "\n",
    "        \"\"\" 생성자(generator)를 학습합니다. \"\"\"\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 랜덤 노이즈(noise) 샘플링\n",
    "        z = torch.normal(mean=0, std=1, size=(imgs.shape[0], latent_dim)).cuda()\n",
    "\n",
    "        # 이미지 생성\n",
    "        generated_imgs = generator(z)\n",
    "\n",
    "        # 생성자(generator)의 손실(loss) 값 계산\n",
    "        g_loss = adversarial_loss(discriminator(generated_imgs), real)\n",
    "\n",
    "        # 생성자(generator) 업데이트\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \"\"\" 판별자(discriminator)를 학습합니다. \"\"\"\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # 판별자(discriminator)의 손실(loss) 값 계산\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), real)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # 판별자(discriminator) 업데이트\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        done = epoch * len(dataloader) + i\n",
    "        if done % sample_interval == 0:\n",
    "            # 생성된 이미지 중에서 25개만 선택하여 5 X 5 격자 이미지에 출력\n",
    "            save_image(generated_imgs.data[:25], f\"{done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # 하나의 epoch이 끝날 때마다 로그(log) 출력\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [Elapsed time: {time.time() - start_time:.2f}s]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('GAN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27188a68af24359f716f42331a05ac5c5dd63b6f781f49d2f8f4a4081f86e482"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
