{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "\n",
    "# 생성자(Generator) 클래스 정의\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # 하나의 블록(block) 정의\n",
    "        def block(input_dim, output_dim, normalize=True):\n",
    "            layers = [nn.Linear(input_dim, output_dim)]\n",
    "            if normalize:\n",
    "                # 배치 정규화(batch normalization) 수행(차원 동일)\n",
    "                layers.append(nn.BatchNorm1d(output_dim, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        # 생성자 모델은 연속적인 여러 개의 블록을 가짐\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, 1 * 28 * 28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.size(0), 1, 28, 28)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판별자(Discriminator) 클래스 정의\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1 * 28 * 28, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    # 이미지에 대한 판별 결과를 반환\n",
    "    def forward(self, img):\n",
    "        flattened = img.view(img.size(0), -1)\n",
    "        output = self.model(flattened)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 불러오기\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize(28),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./dataset', train=True, download=True, transform=transforms_train)\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 및 샘플링\n",
    "# 학습을 위해 생성자와 판별자모델 초기화\n",
    "# 적절한 하이퍼 파라미터 설정\n",
    "\n",
    "# initialize Generator & Discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "adversarial_loss.to(device)\n",
    "\n",
    "# Learning Rate\n",
    "lr = 0.0002\n",
    "\n",
    "# Optimizer for G & D\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [D loss: 0.619810] [G loss: 1.989339] [Elapsed time: 10.64s]\n",
      "[Epoch 1/200] [D loss: 0.260693] [G loss: 1.173720] [Elapsed time: 20.55s]\n",
      "[Epoch 2/200] [D loss: 0.300729] [G loss: 1.171016] [Elapsed time: 29.22s]\n",
      "[Epoch 3/200] [D loss: 0.557267] [G loss: 0.548077] [Elapsed time: 37.47s]\n",
      "[Epoch 4/200] [D loss: 0.723447] [G loss: 0.301939] [Elapsed time: 46.07s]\n",
      "[Epoch 5/200] [D loss: 0.272771] [G loss: 1.395925] [Elapsed time: 58.14s]\n",
      "[Epoch 6/200] [D loss: 0.360840] [G loss: 0.936666] [Elapsed time: 71.91s]\n",
      "[Epoch 7/200] [D loss: 0.319158] [G loss: 3.385357] [Elapsed time: 84.44s]\n",
      "[Epoch 8/200] [D loss: 0.210386] [G loss: 2.618644] [Elapsed time: 94.53s]\n",
      "[Epoch 9/200] [D loss: 0.233650] [G loss: 2.961058] [Elapsed time: 105.21s]\n",
      "[Epoch 10/200] [D loss: 0.207324] [G loss: 2.164872] [Elapsed time: 114.20s]\n",
      "[Epoch 11/200] [D loss: 0.613283] [G loss: 0.480463] [Elapsed time: 124.18s]\n",
      "[Epoch 12/200] [D loss: 0.282079] [G loss: 4.196071] [Elapsed time: 134.41s]\n",
      "[Epoch 13/200] [D loss: 0.225128] [G loss: 2.436913] [Elapsed time: 144.07s]\n",
      "[Epoch 14/200] [D loss: 0.476487] [G loss: 1.145209] [Elapsed time: 154.24s]\n",
      "[Epoch 15/200] [D loss: 0.180758] [G loss: 1.997739] [Elapsed time: 166.73s]\n",
      "[Epoch 16/200] [D loss: 0.358747] [G loss: 0.925970] [Elapsed time: 178.37s]\n",
      "[Epoch 17/200] [D loss: 0.210162] [G loss: 1.424232] [Elapsed time: 190.40s]\n",
      "[Epoch 18/200] [D loss: 0.232018] [G loss: 1.676005] [Elapsed time: 201.87s]\n",
      "[Epoch 19/200] [D loss: 0.160256] [G loss: 1.710596] [Elapsed time: 216.33s]\n",
      "[Epoch 20/200] [D loss: 0.130805] [G loss: 2.104785] [Elapsed time: 225.33s]\n",
      "[Epoch 21/200] [D loss: 0.126016] [G loss: 4.661177] [Elapsed time: 234.19s]\n",
      "[Epoch 22/200] [D loss: 0.191637] [G loss: 1.758744] [Elapsed time: 245.92s]\n",
      "[Epoch 23/200] [D loss: 0.099712] [G loss: 2.807537] [Elapsed time: 255.86s]\n",
      "[Epoch 24/200] [D loss: 0.148167] [G loss: 2.156180] [Elapsed time: 266.69s]\n",
      "[Epoch 25/200] [D loss: 0.187743] [G loss: 2.269940] [Elapsed time: 281.36s]\n",
      "[Epoch 26/200] [D loss: 0.198685] [G loss: 1.345199] [Elapsed time: 292.99s]\n",
      "[Epoch 27/200] [D loss: 0.179140] [G loss: 1.908201] [Elapsed time: 303.30s]\n",
      "[Epoch 28/200] [D loss: 0.144714] [G loss: 2.479571] [Elapsed time: 313.23s]\n",
      "[Epoch 29/200] [D loss: 0.194183] [G loss: 2.406049] [Elapsed time: 323.37s]\n",
      "[Epoch 30/200] [D loss: 0.115344] [G loss: 2.423718] [Elapsed time: 333.80s]\n",
      "[Epoch 31/200] [D loss: 0.192864] [G loss: 1.720339] [Elapsed time: 343.99s]\n",
      "[Epoch 32/200] [D loss: 0.084954] [G loss: 2.972172] [Elapsed time: 356.95s]\n",
      "[Epoch 33/200] [D loss: 0.113218] [G loss: 3.298075] [Elapsed time: 366.59s]\n",
      "[Epoch 34/200] [D loss: 0.124489] [G loss: 2.412006] [Elapsed time: 377.06s]\n",
      "[Epoch 35/200] [D loss: 0.215646] [G loss: 1.680177] [Elapsed time: 386.66s]\n",
      "[Epoch 36/200] [D loss: 0.092798] [G loss: 2.869537] [Elapsed time: 396.64s]\n",
      "[Epoch 37/200] [D loss: 0.153587] [G loss: 3.182539] [Elapsed time: 405.89s]\n",
      "[Epoch 38/200] [D loss: 0.150104] [G loss: 2.270885] [Elapsed time: 415.65s]\n",
      "[Epoch 39/200] [D loss: 0.096484] [G loss: 3.149143] [Elapsed time: 424.61s]\n",
      "[Epoch 40/200] [D loss: 0.171557] [G loss: 3.184876] [Elapsed time: 434.25s]\n",
      "[Epoch 41/200] [D loss: 0.205728] [G loss: 4.982085] [Elapsed time: 442.39s]\n",
      "[Epoch 42/200] [D loss: 0.141399] [G loss: 2.429271] [Elapsed time: 450.55s]\n",
      "[Epoch 43/200] [D loss: 0.194108] [G loss: 4.696817] [Elapsed time: 458.65s]\n",
      "[Epoch 44/200] [D loss: 0.480661] [G loss: 2.346771] [Elapsed time: 466.65s]\n",
      "[Epoch 45/200] [D loss: 0.266887] [G loss: 1.709281] [Elapsed time: 474.69s]\n",
      "[Epoch 46/200] [D loss: 0.316135] [G loss: 4.824888] [Elapsed time: 482.67s]\n",
      "[Epoch 47/200] [D loss: 0.459182] [G loss: 5.722435] [Elapsed time: 490.78s]\n",
      "[Epoch 48/200] [D loss: 0.148016] [G loss: 1.806898] [Elapsed time: 498.72s]\n",
      "[Epoch 49/200] [D loss: 0.085189] [G loss: 2.992361] [Elapsed time: 506.69s]\n",
      "[Epoch 50/200] [D loss: 0.101553] [G loss: 2.692371] [Elapsed time: 514.61s]\n",
      "[Epoch 51/200] [D loss: 0.150421] [G loss: 3.092464] [Elapsed time: 522.61s]\n",
      "[Epoch 52/200] [D loss: 0.194016] [G loss: 4.915899] [Elapsed time: 530.57s]\n",
      "[Epoch 53/200] [D loss: 0.173578] [G loss: 7.697900] [Elapsed time: 538.78s]\n",
      "[Epoch 54/200] [D loss: 0.113293] [G loss: 2.896957] [Elapsed time: 546.72s]\n",
      "[Epoch 55/200] [D loss: 0.228586] [G loss: 3.331989] [Elapsed time: 555.03s]\n",
      "[Epoch 56/200] [D loss: 0.211406] [G loss: 2.163949] [Elapsed time: 563.45s]\n",
      "[Epoch 57/200] [D loss: 0.122662] [G loss: 3.398579] [Elapsed time: 571.45s]\n",
      "[Epoch 58/200] [D loss: 0.103857] [G loss: 2.573551] [Elapsed time: 579.60s]\n",
      "[Epoch 59/200] [D loss: 0.154929] [G loss: 3.228789] [Elapsed time: 587.44s]\n",
      "[Epoch 60/200] [D loss: 0.161018] [G loss: 2.318616] [Elapsed time: 595.56s]\n",
      "[Epoch 61/200] [D loss: 0.065161] [G loss: 3.575874] [Elapsed time: 603.43s]\n",
      "[Epoch 62/200] [D loss: 0.131221] [G loss: 3.157530] [Elapsed time: 611.55s]\n",
      "[Epoch 63/200] [D loss: 0.176517] [G loss: 2.251372] [Elapsed time: 619.93s]\n",
      "[Epoch 64/200] [D loss: 0.173216] [G loss: 4.173701] [Elapsed time: 628.07s]\n",
      "[Epoch 65/200] [D loss: 0.185663] [G loss: 1.810307] [Elapsed time: 635.95s]\n",
      "[Epoch 66/200] [D loss: 0.122778] [G loss: 2.327363] [Elapsed time: 644.05s]\n",
      "[Epoch 67/200] [D loss: 0.176013] [G loss: 1.798791] [Elapsed time: 651.94s]\n",
      "[Epoch 68/200] [D loss: 0.156768] [G loss: 2.923383] [Elapsed time: 660.06s]\n",
      "[Epoch 69/200] [D loss: 0.144433] [G loss: 2.462041] [Elapsed time: 668.03s]\n",
      "[Epoch 70/200] [D loss: 0.240794] [G loss: 1.657744] [Elapsed time: 676.40s]\n",
      "[Epoch 71/200] [D loss: 0.150476] [G loss: 2.227117] [Elapsed time: 684.50s]\n",
      "[Epoch 72/200] [D loss: 0.200952] [G loss: 2.689739] [Elapsed time: 692.55s]\n",
      "[Epoch 73/200] [D loss: 0.145295] [G loss: 4.458219] [Elapsed time: 700.45s]\n",
      "[Epoch 74/200] [D loss: 0.222905] [G loss: 2.072915] [Elapsed time: 708.91s]\n",
      "[Epoch 75/200] [D loss: 0.127275] [G loss: 4.348402] [Elapsed time: 719.14s]\n",
      "[Epoch 76/200] [D loss: 0.098239] [G loss: 3.238369] [Elapsed time: 729.27s]\n",
      "[Epoch 77/200] [D loss: 0.126179] [G loss: 2.098092] [Elapsed time: 737.44s]\n",
      "[Epoch 78/200] [D loss: 0.334001] [G loss: 2.466720] [Elapsed time: 746.43s]\n",
      "[Epoch 79/200] [D loss: 0.204718] [G loss: 1.875568] [Elapsed time: 756.01s]\n",
      "[Epoch 80/200] [D loss: 0.136704] [G loss: 2.460566] [Elapsed time: 766.07s]\n",
      "[Epoch 81/200] [D loss: 0.132204] [G loss: 2.847897] [Elapsed time: 775.33s]\n",
      "[Epoch 82/200] [D loss: 0.111321] [G loss: 3.466304] [Elapsed time: 784.05s]\n",
      "[Epoch 83/200] [D loss: 0.123181] [G loss: 3.957472] [Elapsed time: 792.75s]\n",
      "[Epoch 84/200] [D loss: 0.136893] [G loss: 2.996291] [Elapsed time: 802.36s]\n",
      "[Epoch 85/200] [D loss: 0.193328] [G loss: 3.165536] [Elapsed time: 811.89s]\n",
      "[Epoch 86/200] [D loss: 0.122330] [G loss: 2.323694] [Elapsed time: 823.53s]\n",
      "[Epoch 87/200] [D loss: 0.174353] [G loss: 3.960441] [Elapsed time: 832.33s]\n",
      "[Epoch 88/200] [D loss: 0.166539] [G loss: 1.743425] [Elapsed time: 841.89s]\n",
      "[Epoch 89/200] [D loss: 0.155027] [G loss: 3.074950] [Elapsed time: 850.69s]\n",
      "[Epoch 90/200] [D loss: 0.179116] [G loss: 3.274412] [Elapsed time: 859.65s]\n",
      "[Epoch 91/200] [D loss: 0.086268] [G loss: 3.039106] [Elapsed time: 868.63s]\n",
      "[Epoch 92/200] [D loss: 0.136819] [G loss: 2.646909] [Elapsed time: 877.97s]\n",
      "[Epoch 93/200] [D loss: 0.186046] [G loss: 3.285301] [Elapsed time: 888.05s]\n",
      "[Epoch 94/200] [D loss: 0.071380] [G loss: 3.159525] [Elapsed time: 897.13s]\n",
      "[Epoch 95/200] [D loss: 0.151198] [G loss: 3.299297] [Elapsed time: 908.06s]\n",
      "[Epoch 96/200] [D loss: 0.072735] [G loss: 3.165537] [Elapsed time: 919.51s]\n",
      "[Epoch 97/200] [D loss: 0.137191] [G loss: 3.643525] [Elapsed time: 929.29s]\n",
      "[Epoch 98/200] [D loss: 0.145235] [G loss: 2.672058] [Elapsed time: 939.45s]\n",
      "[Epoch 99/200] [D loss: 0.117735] [G loss: 4.115996] [Elapsed time: 949.46s]\n",
      "[Epoch 100/200] [D loss: 0.091673] [G loss: 3.312147] [Elapsed time: 958.38s]\n",
      "[Epoch 101/200] [D loss: 0.187930] [G loss: 2.419130] [Elapsed time: 967.90s]\n",
      "[Epoch 102/200] [D loss: 0.147249] [G loss: 4.321087] [Elapsed time: 977.03s]\n",
      "[Epoch 103/200] [D loss: 0.168211] [G loss: 3.093445] [Elapsed time: 985.61s]\n",
      "[Epoch 104/200] [D loss: 0.150581] [G loss: 3.448054] [Elapsed time: 995.18s]\n",
      "[Epoch 105/200] [D loss: 0.211023] [G loss: 2.439237] [Elapsed time: 1004.64s]\n",
      "[Epoch 106/200] [D loss: 0.155590] [G loss: 4.644272] [Elapsed time: 1013.65s]\n",
      "[Epoch 107/200] [D loss: 0.053507] [G loss: 3.870691] [Elapsed time: 1022.80s]\n",
      "[Epoch 108/200] [D loss: 0.046545] [G loss: 3.698122] [Elapsed time: 1031.42s]\n",
      "[Epoch 109/200] [D loss: 0.323572] [G loss: 7.818755] [Elapsed time: 1042.61s]\n",
      "[Epoch 110/200] [D loss: 0.074050] [G loss: 4.057583] [Elapsed time: 1052.13s]\n",
      "[Epoch 111/200] [D loss: 0.144696] [G loss: 4.509466] [Elapsed time: 1061.89s]\n",
      "[Epoch 112/200] [D loss: 0.076199] [G loss: 3.496010] [Elapsed time: 1071.29s]\n",
      "[Epoch 113/200] [D loss: 0.051518] [G loss: 3.674412] [Elapsed time: 1081.00s]\n",
      "[Epoch 114/200] [D loss: 0.080849] [G loss: 3.153706] [Elapsed time: 1089.60s]\n",
      "[Epoch 115/200] [D loss: 0.111580] [G loss: 3.901149] [Elapsed time: 1098.90s]\n",
      "[Epoch 116/200] [D loss: 0.039285] [G loss: 4.294240] [Elapsed time: 1109.10s]\n",
      "[Epoch 117/200] [D loss: 0.054827] [G loss: 2.855100] [Elapsed time: 1121.52s]\n",
      "[Epoch 118/200] [D loss: 0.114420] [G loss: 4.822638] [Elapsed time: 1133.42s]\n",
      "[Epoch 119/200] [D loss: 0.105281] [G loss: 2.824579] [Elapsed time: 1144.35s]\n",
      "[Epoch 120/200] [D loss: 0.027257] [G loss: 4.362313] [Elapsed time: 1153.26s]\n",
      "[Epoch 121/200] [D loss: 0.077757] [G loss: 2.704547] [Elapsed time: 1162.71s]\n",
      "[Epoch 122/200] [D loss: 0.061467] [G loss: 3.708770] [Elapsed time: 1172.21s]\n",
      "[Epoch 123/200] [D loss: 0.062588] [G loss: 3.118497] [Elapsed time: 1182.56s]\n",
      "[Epoch 124/200] [D loss: 0.081958] [G loss: 3.534899] [Elapsed time: 1192.08s]\n",
      "[Epoch 125/200] [D loss: 0.182262] [G loss: 8.518404] [Elapsed time: 1201.54s]\n",
      "[Epoch 126/200] [D loss: 0.070770] [G loss: 5.227143] [Elapsed time: 1212.76s]\n",
      "[Epoch 127/200] [D loss: 0.096584] [G loss: 3.165926] [Elapsed time: 1226.82s]\n",
      "[Epoch 128/200] [D loss: 0.144815] [G loss: 4.118789] [Elapsed time: 1242.37s]\n",
      "[Epoch 129/200] [D loss: 0.091844] [G loss: 2.959676] [Elapsed time: 1252.83s]\n",
      "[Epoch 130/200] [D loss: 0.090679] [G loss: 3.984186] [Elapsed time: 1263.46s]\n",
      "[Epoch 131/200] [D loss: 0.090356] [G loss: 4.091345] [Elapsed time: 1274.60s]\n",
      "[Epoch 132/200] [D loss: 0.254489] [G loss: 2.794509] [Elapsed time: 1286.56s]\n",
      "[Epoch 133/200] [D loss: 0.125635] [G loss: 3.364021] [Elapsed time: 1296.37s]\n",
      "[Epoch 134/200] [D loss: 0.103334] [G loss: 5.858585] [Elapsed time: 1306.01s]\n",
      "[Epoch 135/200] [D loss: 0.148297] [G loss: 4.731649] [Elapsed time: 1314.96s]\n",
      "[Epoch 136/200] [D loss: 0.193846] [G loss: 4.964377] [Elapsed time: 1324.12s]\n",
      "[Epoch 137/200] [D loss: 0.093111] [G loss: 3.797748] [Elapsed time: 1334.08s]\n",
      "[Epoch 138/200] [D loss: 0.088610] [G loss: 4.244864] [Elapsed time: 1343.25s]\n",
      "[Epoch 139/200] [D loss: 0.132260] [G loss: 3.820100] [Elapsed time: 1351.73s]\n",
      "[Epoch 140/200] [D loss: 0.103788] [G loss: 5.939390] [Elapsed time: 1360.55s]\n",
      "[Epoch 141/200] [D loss: 0.121748] [G loss: 2.500736] [Elapsed time: 1370.19s]\n",
      "[Epoch 142/200] [D loss: 0.179935] [G loss: 2.701052] [Elapsed time: 1378.77s]\n",
      "[Epoch 143/200] [D loss: 0.094493] [G loss: 3.505563] [Elapsed time: 1387.52s]\n",
      "[Epoch 144/200] [D loss: 0.155805] [G loss: 2.406779] [Elapsed time: 1396.19s]\n",
      "[Epoch 145/200] [D loss: 0.061608] [G loss: 5.158247] [Elapsed time: 1404.15s]\n",
      "[Epoch 146/200] [D loss: 0.042601] [G loss: 3.159138] [Elapsed time: 1412.45s]\n",
      "[Epoch 147/200] [D loss: 0.242690] [G loss: 2.640232] [Elapsed time: 1423.51s]\n",
      "[Epoch 148/200] [D loss: 0.198352] [G loss: 6.428487] [Elapsed time: 1433.86s]\n",
      "[Epoch 149/200] [D loss: 0.086249] [G loss: 3.677172] [Elapsed time: 1444.60s]\n",
      "[Epoch 150/200] [D loss: 0.101694] [G loss: 2.003283] [Elapsed time: 1455.67s]\n",
      "[Epoch 151/200] [D loss: 0.134995] [G loss: 4.989804] [Elapsed time: 1465.26s]\n",
      "[Epoch 152/200] [D loss: 0.142962] [G loss: 4.030977] [Elapsed time: 1475.59s]\n",
      "[Epoch 153/200] [D loss: 0.105367] [G loss: 3.299594] [Elapsed time: 1485.41s]\n",
      "[Epoch 154/200] [D loss: 0.136829] [G loss: 3.569738] [Elapsed time: 1496.82s]\n",
      "[Epoch 155/200] [D loss: 0.178901] [G loss: 3.211874] [Elapsed time: 1506.39s]\n",
      "[Epoch 156/200] [D loss: 0.216698] [G loss: 4.417673] [Elapsed time: 1517.05s]\n",
      "[Epoch 157/200] [D loss: 0.047516] [G loss: 3.236615] [Elapsed time: 1526.73s]\n",
      "[Epoch 158/200] [D loss: 0.131425] [G loss: 2.288921] [Elapsed time: 1536.24s]\n",
      "[Epoch 159/200] [D loss: 0.050381] [G loss: 3.780112] [Elapsed time: 1545.84s]\n",
      "[Epoch 160/200] [D loss: 0.107108] [G loss: 3.976890] [Elapsed time: 1555.28s]\n",
      "[Epoch 161/200] [D loss: 0.057823] [G loss: 4.995195] [Elapsed time: 1565.21s]\n",
      "[Epoch 162/200] [D loss: 0.111708] [G loss: 3.401171] [Elapsed time: 1575.46s]\n",
      "[Epoch 163/200] [D loss: 0.063628] [G loss: 7.156208] [Elapsed time: 1585.80s]\n",
      "[Epoch 164/200] [D loss: 0.144245] [G loss: 4.789563] [Elapsed time: 1596.30s]\n",
      "[Epoch 165/200] [D loss: 0.090072] [G loss: 2.594441] [Elapsed time: 1609.51s]\n",
      "[Epoch 166/200] [D loss: 0.077045] [G loss: 3.353603] [Elapsed time: 1622.39s]\n",
      "[Epoch 167/200] [D loss: 0.099951] [G loss: 4.525778] [Elapsed time: 1634.44s]\n",
      "[Epoch 168/200] [D loss: 0.239708] [G loss: 6.312233] [Elapsed time: 1644.99s]\n",
      "[Epoch 169/200] [D loss: 0.089633] [G loss: 4.007466] [Elapsed time: 1655.64s]\n",
      "[Epoch 170/200] [D loss: 0.116733] [G loss: 2.838656] [Elapsed time: 1667.30s]\n",
      "[Epoch 171/200] [D loss: 0.106180] [G loss: 3.337934] [Elapsed time: 1678.75s]\n",
      "[Epoch 172/200] [D loss: 0.092500] [G loss: 4.497094] [Elapsed time: 1690.88s]\n",
      "[Epoch 173/200] [D loss: 0.138475] [G loss: 4.571473] [Elapsed time: 1703.13s]\n",
      "[Epoch 174/200] [D loss: 0.115692] [G loss: 3.024588] [Elapsed time: 1712.71s]\n",
      "[Epoch 175/200] [D loss: 0.122331] [G loss: 5.867408] [Elapsed time: 1723.22s]\n",
      "[Epoch 176/200] [D loss: 0.080491] [G loss: 3.041042] [Elapsed time: 1733.80s]\n",
      "[Epoch 177/200] [D loss: 0.153723] [G loss: 3.513353] [Elapsed time: 1745.21s]\n",
      "[Epoch 178/200] [D loss: 0.105239] [G loss: 2.493955] [Elapsed time: 1755.32s]\n",
      "[Epoch 179/200] [D loss: 0.089172] [G loss: 4.484561] [Elapsed time: 1765.18s]\n",
      "[Epoch 180/200] [D loss: 0.142381] [G loss: 3.923988] [Elapsed time: 1780.16s]\n",
      "[Epoch 181/200] [D loss: 0.176013] [G loss: 2.723633] [Elapsed time: 1795.37s]\n",
      "[Epoch 182/200] [D loss: 0.251839] [G loss: 2.488090] [Elapsed time: 1808.84s]\n",
      "[Epoch 183/200] [D loss: 0.084729] [G loss: 4.052144] [Elapsed time: 1825.11s]\n",
      "[Epoch 184/200] [D loss: 0.123755] [G loss: 2.327277] [Elapsed time: 1842.26s]\n",
      "[Epoch 185/200] [D loss: 0.150077] [G loss: 2.183268] [Elapsed time: 1856.67s]\n",
      "[Epoch 186/200] [D loss: 0.137317] [G loss: 6.796675] [Elapsed time: 1866.82s]\n",
      "[Epoch 187/200] [D loss: 0.095319] [G loss: 4.862632] [Elapsed time: 1876.81s]\n",
      "[Epoch 188/200] [D loss: 0.097926] [G loss: 2.447875] [Elapsed time: 1886.40s]\n",
      "[Epoch 189/200] [D loss: 0.141458] [G loss: 3.548631] [Elapsed time: 1896.27s]\n",
      "[Epoch 190/200] [D loss: 0.064915] [G loss: 3.898455] [Elapsed time: 1904.74s]\n",
      "[Epoch 191/200] [D loss: 0.086673] [G loss: 3.492689] [Elapsed time: 1913.55s]\n",
      "[Epoch 192/200] [D loss: 0.510413] [G loss: 7.037796] [Elapsed time: 1923.58s]\n",
      "[Epoch 193/200] [D loss: 0.083033] [G loss: 3.521578] [Elapsed time: 1932.63s]\n",
      "[Epoch 194/200] [D loss: 0.133055] [G loss: 4.251246] [Elapsed time: 1942.56s]\n",
      "[Epoch 195/200] [D loss: 0.125126] [G loss: 5.225632] [Elapsed time: 1953.03s]\n",
      "[Epoch 196/200] [D loss: 0.114957] [G loss: 3.791329] [Elapsed time: 1961.35s]\n",
      "[Epoch 197/200] [D loss: 0.142607] [G loss: 4.532178] [Elapsed time: 1970.09s]\n",
      "[Epoch 198/200] [D loss: 0.118118] [G loss: 9.201175] [Elapsed time: 1979.77s]\n",
      "[Epoch 199/200] [D loss: 0.111317] [G loss: 3.337937] [Elapsed time: 1988.67s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 200 # 학습의 횟수(epoch) 설정\n",
    "sample_interval = 5000 # 몇 번의 배치(batch)마다 결과를 출력할 것인지 설정\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "        # 진짜(real) 이미지와 가짜(fake) 이미지에 대한 정답 레이블 생성\n",
    "        real = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(1.0) # 진짜(real): 1\n",
    "        fake = torch.cuda.FloatTensor(imgs.size(0), 1).fill_(0.0) # 가짜(fake): 0\n",
    "\n",
    "        real_imgs = imgs.to(device)\n",
    "\n",
    "        \"\"\" 생성자(generator)를 학습합니다. \"\"\"\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # 랜덤 노이즈(noise) 샘플링\n",
    "        z = torch.normal(mean=0, std=1, size=(imgs.shape[0], latent_dim)).to(device)\n",
    "\n",
    "        # 이미지 생성\n",
    "        generated_imgs = generator(z)\n",
    "\n",
    "        # 생성자(generator)의 손실(loss) 값 계산\n",
    "        g_loss = adversarial_loss(discriminator(generated_imgs), real)\n",
    "\n",
    "        # 생성자(generator) 업데이트\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        \"\"\" 판별자(discriminator)를 학습합니다. \"\"\"\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # 판별자(discriminator)의 손실(loss) 값 계산\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs), real)\n",
    "        fake_loss = adversarial_loss(discriminator(generated_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        # 판별자(discriminator) 업데이트\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        done = epoch * len(dataloader) + i\n",
    "        if done % sample_interval == 0:\n",
    "            # 생성된 이미지 중에서 25개만 선택하여 5 X 5 격자 이미지에 출력\n",
    "            save_image(generated_imgs.data[:25], f\"{done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "    # 하나의 epoch이 끝날 때마다 로그(log) 출력\n",
    "    print(f\"[Epoch {epoch}/{n_epochs}] [D loss: {d_loss.item():.6f}] [G loss: {g_loss.item():.6f}] [Elapsed time: {time.time() - start_time:.2f}s]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('GAN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27188a68af24359f716f42331a05ac5c5dd63b6f781f49d2f8f4a4081f86e482"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
